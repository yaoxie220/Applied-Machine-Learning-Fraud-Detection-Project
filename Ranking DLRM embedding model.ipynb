{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1932f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.keras.layers import Layer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66840cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('fraudTrain.csv')\n",
    "train_df.drop(['Unnamed: 0'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5064b95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories for category: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/demetriosfassois/Documents/Columbia/EECSE6895/Project/6895_env2/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Store log columns as new features\n",
    "train_df[\"log_amount\"] = np.log(train_df[\"amt\"])\n",
    "train_df[\"log_city_pop\"] = np.log(train_df[\"city_pop\"])\n",
    "# Create age column from date of birth\n",
    "train_df[\"age\"] = 2023 - pd.to_datetime(train_df[\"dob\"]).dt.year\n",
    "# Create hour and month columns from the transaction datetime column\n",
    "train_df[\"hour\"] = pd.to_datetime(train_df[\"trans_date_trans_time\"]).dt.hour\n",
    "train_df[\"month\"] = pd.to_datetime(train_df[\"trans_date_trans_time\"]).dt.month\n",
    "#### Combine latitude and longitude columns and bucketize them \n",
    "lat_bins = [15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]\n",
    "long_bins = [-170, -150, -130, -110, -90, -70, -50]\n",
    "train_df['lat_binned'] = pd.cut(train_df['lat'], lat_bins)\n",
    "train_df['long_binned'] = pd.cut(train_df['long'], long_bins)\n",
    "train_df['merch_lat_binned'] = pd.cut(train_df['merch_lat'], lat_bins)\n",
    "train_df['merch_long_binned'] = pd.cut(train_df['merch_long'], long_bins)\n",
    "train_df[\"long_lat_binned\"] = list(zip(train_df[\"lat_binned\"], train_df[\"long_binned\"]))\n",
    "train_df[\"merch_long_lat_binned\"] = list(zip(train_df[\"merch_lat_binned\"], train_df[\"merch_long_binned\"]))\n",
    "# Encode the frequency distribution of the credit card numbers\n",
    "cc_num_frequency_counts = train_df[\"cc_num\"].value_counts()\n",
    "train_df[\"cc_num_frequency\"] = train_df[\"cc_num\"].map(cc_num_frequency_counts)\n",
    "# Category and gender can be encoded using one-hot encoding\n",
    "print(f\"Unique categories for category: {len(train_df.category.unique())}\")\n",
    "ohe_features = [\"gender\"]\n",
    "# Encode lower dimensional categorical features using one-hot encoding\n",
    "ohe = OneHotEncoder(handle_unknown=\"error\", sparse=False)\n",
    "ohe_encoded_X_train = pd.DataFrame(ohe.fit_transform(train_df[ohe_features]))\n",
    "ohe_encoded_features = list(ohe.get_feature_names_out())\n",
    "ohe_encoded_X_train.columns = ohe_encoded_features\n",
    "train_df = pd.concat([train_df, ohe_encoded_X_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c22ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final features\n",
    "sparse_feature_names = [\"merchant\", \"category\", \"city\", \"state\", \"zip\", \"job\", \n",
    "                        \"long_lat_binned\", \"merch_long_lat_binned\"]\n",
    "dense_feature_names = [\"log_amount\", \"log_city_pop\", \"age\", \"hour\", \"month\", \"cc_num_frequency\"] + \\\n",
    "                        ohe_encoded_features\n",
    "sparse_features = train_df[sparse_feature_names]\n",
    "dense_features = train_df[dense_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3181f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabularies dictionary\n",
    "vocabularies = {}\n",
    "for feat in sparse_feature_names:\n",
    "    sparse_feature = train_df[feat].astype(\"str\")\n",
    "    feat_vocab = sparse_feature.unique().tolist()\n",
    "    vocabularies[feat] = feat_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "044667f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = {}\n",
    "\n",
    "embedding_dims[\"merchant\"] = 16\n",
    "embedding_dims[\"category\"] = 16\n",
    "embedding_dims[\"city\"] = 16\n",
    "embedding_dims[\"state\"] = 16\n",
    "embedding_dims[\"zip\"] = 16\n",
    "embedding_dims[\"job\"] = 16\n",
    "embedding_dims[\"long_lat_binned\"] = 16\n",
    "embedding_dims[\"merch_long_lat_binned\"] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab175a1e",
   "metadata": {},
   "source": [
    "From tfrs.experimental.models.Ranking docs: https://www.tensorflow.org/recommenders/api_docs/python/tfrs/experimental/models/Ranking\n",
    "\n",
    "embedding_layer:\tThe embedding layer is applied to categorical features. It expects a string-to-tensor (or SparseTensor/RaggedTensor) dict as an input, and outputs a dictionary of string-to-tensor of feature_name, embedded_value pairs. {feature_name_i: tensor_i} -> {feature_name_i: emb(tensor_i)}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4485aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(Layer):\n",
    "\n",
    "    def __init__(self, sparse_feature_names, vocabularies, embedding_dims):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.sparse_feature_names = sparse_feature_names\n",
    "        self.vocabularies = vocabularies\n",
    "        self.embedding_dims = embedding_dims\n",
    "\n",
    "    def build(self, input_shape):  # Create the state of the layer (weights)\n",
    "        self.embbedings = {}\n",
    "        for feature_name in self.sparse_feature_names:\n",
    "            self.embbedings[feature_name] = tf.keras.layers.Embedding(len(self.vocabularies[feature_name]) + 1, self.embedding_dims[feature_name])\n",
    "            \n",
    "\n",
    "    def call(self, sparse_features_dict):  # Defines the computation from inputs to outputs\n",
    "        outputs = {}\n",
    "        for feature_name, embedding in self.embbedings.items():\n",
    "            feature_output = embedding(sparse_features_dict[feature_name])\n",
    "            outputs[feature_name] = feature_output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6404ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = EmbeddingLayer(sparse_feature_names, vocabularies, embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c0f511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfrs.experimental.models.Ranking(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ebf2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6af001e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.5029111776656126, 1: 86.37589928057554}\n"
     ]
    }
   ],
   "source": [
    "target_variable = \"is_fraud\"\n",
    "\n",
    "classes = np.unique(train_df[target_variable])\n",
    "class_weights = compute_class_weight(class_weight = 'balanced', \n",
    "                                     classes = classes, \n",
    "                                     y = train_df[target_variable])\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "print(\"Class Weights:\", class_weight_dict)\n",
    "\n",
    "class_weights_tensor = tf.constant(list(class_weight_dict.values()), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59389a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for i in list(range(0, train_df.shape[0], batch_size)):\n",
    "    # Create inputs dict\n",
    "    inputs = {}\n",
    "    \n",
    "    # Create label\n",
    "    y = train_df[i: i+batch_size][target_variable]\n",
    "    label_tensor = tf.convert_to_tensor(y)\n",
    "    inputs[\"label_tensor\"] = label_tensor\n",
    "\n",
    "    # Create sparse features\n",
    "    sparse_features_dict = {}\n",
    "    for feat in sparse_feature_names:\n",
    "        sparse_feature = train_df[i: i+batch_size][feat].astype(str)\n",
    "        feat_tensor = tf.convert_to_tensor(sparse_feature)\n",
    "        feat_lookup_layer = tf.keras.layers.StringLookup(vocabulary=vocabularies[feat])\n",
    "        feat_input = feat_lookup_layer(feat_tensor)\n",
    "        sparse_features_dict[feat] = feat_input\n",
    "    inputs[\"sparse_features\"] = sparse_features_dict\n",
    "    \n",
    "    # Create dense features\n",
    "    dense_features = train_df[i: i+batch_size][dense_feature_names]\n",
    "    inputs[\"dense_features\"] = tf.convert_to_tensor(dense_features)\n",
    "    \n",
    "    # Create sample weights\n",
    "    sample_weight = tf.gather(class_weights_tensor, label_tensor)\n",
    "    inputs[\"sample_weight\"] = sample_weight\n",
    "    \n",
    "    model.fit(inputs, label_tensor, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bee60383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239281"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef108e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "122087c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"fraudTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "731ecda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store log columns as new features\n",
    "test_df[\"log_amount\"] = np.log(test_df[\"amt\"])\n",
    "test_df[\"log_city_pop\"] = np.log(test_df[\"city_pop\"])\n",
    "# Create age column from date of birth\n",
    "test_df[\"age\"] = 2023 - pd.to_datetime(test_df[\"dob\"]).dt.year\n",
    "# Create hour and month columns from the transaction datetime column\n",
    "test_df[\"hour\"] = pd.to_datetime(test_df[\"trans_date_trans_time\"]).dt.hour\n",
    "test_df[\"month\"] = pd.to_datetime(test_df[\"trans_date_trans_time\"]).dt.month\n",
    "#### Combine latitude and longitude columns and bucketize them \n",
    "test_df['lat_binned'] = pd.cut(test_df['lat'], lat_bins)\n",
    "test_df['long_binned'] = pd.cut(test_df['long'], long_bins)\n",
    "test_df['merch_lat_binned'] = pd.cut(test_df['merch_lat'], lat_bins)\n",
    "test_df['merch_long_binned'] = pd.cut(test_df['merch_long'], long_bins)\n",
    "test_df[\"long_lat_binned\"] = list(zip(test_df[\"lat_binned\"], test_df[\"long_binned\"]))\n",
    "test_df[\"merch_long_lat_binned\"] = list(zip(test_df[\"merch_lat_binned\"], test_df[\"merch_long_binned\"]))\n",
    "# Encode the frequency distribution of the credit card numbers\n",
    "test_df[\"cc_num_frequency\"] = test_df[\"cc_num\"].map(cc_num_frequency_counts)\n",
    "# Indicate new credit card numbers in the test data by setting their frequency to -1\n",
    "test_df[\"cc_num_frequency\"].fillna(-1, inplace=True)\n",
    "\n",
    "# Encode lower dimensional categorical features using one-hot encoding\n",
    "ohe_encoded_X_test = pd.DataFrame(ohe.transform(test_df[ohe_features]))\n",
    "ohe_encoded_X_test.columns = ohe_encoded_features\n",
    "test_df = pd.concat([test_df, ohe_encoded_X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in list(range(0, test_df.shape[0], batch_size)):\n",
    "    # Create inputs dict\n",
    "    inputs = {}\n",
    "    \n",
    "    # Create label\n",
    "    y = test_df[i: i+batch_size][target_variable]\n",
    "    label_tensor = tf.convert_to_tensor(y)\n",
    "    inputs[\"label_tensor\"] = label_tensor\n",
    "\n",
    "    # Create sparse features\n",
    "    sparse_features_dict = {}\n",
    "    for feat in sparse_feature_names:\n",
    "        sparse_feature = test_df[i: i+batch_size][feat].astype(str)\n",
    "        feat_tensor = tf.convert_to_tensor(sparse_feature)\n",
    "        feat_lookup_layer = tf.keras.layers.StringLookup(vocabulary=vocabularies[feat])\n",
    "        feat_input = feat_lookup_layer(feat_tensor)\n",
    "        sparse_features_dict[feat] = feat_input\n",
    "    inputs[\"sparse_features\"] = sparse_features_dict\n",
    "    \n",
    "    # Create dense features\n",
    "    dense_features = test_df[i: i+batch_size][dense_feature_names]\n",
    "    inputs[\"dense_features\"] = tf.convert_to_tensor(dense_features)\n",
    "    \n",
    "    pred = model.predict(inputs, verbose=0)\n",
    "    preds.extend(list(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y = test_df[target_variable]\n",
    "true_y = true_y.values\n",
    "\n",
    "preds = np.array(preds)\n",
    "\n",
    "pred_y = (preds >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7ba9fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.5862\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test AUC: {round(roc_auc_score(true_y, preds), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb5c18f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       1.00      0.08      0.14      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       1.00      0.54      0.57    555719\n",
      "weighted avg       1.00      1.00      0.99    555719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report: \\n\")\n",
    "print(classification_report(true_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220ad03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0938720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27d3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d2ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3dabe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a21f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47721db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6895_env2",
   "language": "python",
   "name": "6895_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
